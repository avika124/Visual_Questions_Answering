{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Question Answering with ViLT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1\n",
      "MPS available: True\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Transformers imports\n",
    "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Load MS-COCO Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 214354 questions\n",
      "Loaded 214354 annotations\n"
     ]
    }
   ],
   "source": [
    "# Load questions\n",
    "\n",
    "with open('v2_OpenEnded_mscoco_val2014_questions.json', 'r') as f:\n",
    "    questions_data = json.load(f)\n",
    "\n",
    "# Load annotations\n",
    "with open('v2_mscoco_val2014_annotations.json', 'r') as f:\n",
    "    annotations_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(questions_data['questions'])} questions\")\n",
    "print(f\"Loaded {len(annotations_data['annotations'])} annotations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Prepare Dataset for ViLT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 30000/30000 [00:12<00:00, 2321.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Collected 30000 valid samples\n",
      "  Training: 24000 samples\n",
      "  Validation: 6000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare image-question-answer triplets\n",
    "N = 30000  \n",
    "vilt_data = []\n",
    "\n",
    "for i in tqdm(range(min(N, len(questions_data['questions'])))):\n",
    "    question_obj = questions_data['questions'][i]\n",
    "    image_id = question_obj['image_id']\n",
    "    question_id = question_obj['question_id']\n",
    "    img_filename = f\"COCO_val2014_{image_id:012d}.jpg\"\n",
    "    img_path = f\"val2014/{img_filename}\"\n",
    "    \n",
    "    # Check if image exists\n",
    "    if os.path.exists(img_path):\n",
    "        # Get annotation\n",
    "        ann = next((a for a in annotations_data['annotations'] \n",
    "                   if a['question_id'] == question_id), None)\n",
    "        if ann:\n",
    "            # Get most common answer\n",
    "            answers = [ans['answer'] for ans in ann['answers']]\n",
    "            most_common = max(set(answers), key=answers.count)\n",
    "            \n",
    "            vilt_data.append({\n",
    "                'image_path': img_path,\n",
    "                'question': question_obj['question'],\n",
    "                'answer': most_common,\n",
    "                'all_answers': answers\n",
    "            })\n",
    "\n",
    "print(f\"\\n Collected {len(vilt_data)} valid samples\")\n",
    "\n",
    "# Split into train/val (80/20)\n",
    "split_idx = int(0.8 * len(vilt_data))\n",
    "train_data = vilt_data[:split_idx]\n",
    "val_data = vilt_data[split_idx:]\n",
    "\n",
    "print(f\"  Training: {len(train_data)} samples\")\n",
    "print(f\"  Validation: {len(val_data)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Build Answer Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer vocabulary: 3000 classes\n",
      "  Coverage: 98.5%\n",
      "\n",
      "Top 10 most common answers:\n",
      "  no: 4574\n",
      "  yes: 4431\n",
      "  1: 671\n",
      "  2: 646\n",
      "  white: 496\n",
      "  blue: 346\n",
      "  3: 321\n",
      "  red: 301\n",
      "  black: 281\n",
      "  0: 265\n"
     ]
    }
   ],
   "source": [
    "# Build answer vocabulary from training data\n",
    "train_answers = [d['answer'] for d in train_data]\n",
    "answer_freq = Counter(train_answers)\n",
    "\n",
    "# Use top 3000 answers for better coverage\n",
    "top_k = 3000\n",
    "top_answers = [ans for ans, _ in answer_freq.most_common(top_k)]\n",
    "answer_to_id = {ans: idx for idx, ans in enumerate(top_answers)}\n",
    "id_to_answer = {idx: ans for ans, idx in answer_to_id.items()}\n",
    "\n",
    "print(f\"Answer vocabulary: {len(answer_to_id)} classes\")\n",
    "coverage = sum(1 for d in train_data if d['answer'] in answer_to_id) / len(train_data)\n",
    "print(f\"  Coverage: {coverage*100:.1f}%\")\n",
    "\n",
    "# Show distribution\n",
    "print(f\"\\nTop 10 most common answers:\")\n",
    "for ans, count in answer_freq.most_common(10):\n",
    "    print(f\"  {ans}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: PyTorch Dataset for ViLT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 24000 samples\n",
      "Val dataset: 6000 samples\n"
     ]
    }
   ],
   "source": [
    "class VQAViLTDataset(Dataset):\n",
    "    \"\"\"Dataset for ViLT model\"\"\"\n",
    "    def __init__(self, data, processor, answer_to_id):\n",
    "        self.data = data\n",
    "        self.processor = processor\n",
    "        self.answer_to_id = answer_to_id\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        try:\n",
    "            # Load image and RESIZE to fixed size\n",
    "            image = Image.open(item['image_path']).convert('RGB')\n",
    "            image = image.resize((384, 384))  # <-- ADD THIS LINE\n",
    "        except:\n",
    "            # Fallback to blank image\n",
    "            image = Image.new('RGB', (384, 384), color='white')  # <-- Also update this\n",
    "        \n",
    "        question = item['question']\n",
    "        answer = item['answer']\n",
    "        \n",
    "        # Process with ViLT processor\n",
    "        encoding = self.processor(\n",
    "            image, \n",
    "            question,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=40,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Remove batch dimension\n",
    "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "        \n",
    "        # Add label\n",
    "        # Add label as one-hot vector (ViLT expects this format)\n",
    "        num_labels = len(self.answer_to_id)\n",
    "        labels = torch.zeros(num_labels)\n",
    "        if answer in self.answer_to_id:\n",
    "            labels[self.answer_to_id[answer]] = 1.0\n",
    "        \n",
    "        encoding['labels'] = labels\n",
    "        \n",
    "        return encoding\n",
    "        \n",
    "        \n",
    "\n",
    "# Load ViLT processor\n",
    "\n",
    "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = VQAViLTDataset(train_data, processor, answer_to_id)\n",
    "val_dataset = VQAViLTDataset(val_data, processor, answer_to_id)\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"Val dataset: {len(val_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train batches: 1500\n",
      " Val batches: 375\n"
     ]
    }
   ],
   "source": [
    "# Collate function to handle batching\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([item['pixel_values'] for item in batch]),\n",
    "        'input_ids': torch.stack([item['input_ids'] for item in batch]),\n",
    "        'attention_mask': torch.stack([item['attention_mask'] for item in batch]),\n",
    "        'labels': torch.stack([item['labels'] for item in batch])  # Now [batch, 3000]\n",
    "    }\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 16  \n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn \n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn  \n",
    ")\n",
    "\n",
    "print(f\" Train batches: {len(train_loader)}\")\n",
    "print(f\" Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Load ViLT Model and Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViltForQuestionAnswering were not initialized from the model checkpoint at dandelin/vilt-b32-finetuned-vqa and are newly initialized because the shapes did not match:\n",
      "- classifier.3.weight: found shape torch.Size([3129, 1536]) in the checkpoint and torch.Size([3000, 1536]) in the model instantiated\n",
      "- classifier.3.bias: found shape torch.Size([3129]) in the checkpoint and torch.Size([3000]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on mps\n",
      "\n",
      "Training Configuration:\n",
      "  Epochs: 5\n",
      "  Learning rate: 5e-05\n",
      "  Training steps: 7500\n",
      "  Warmup steps: 750\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained ViLT model\n",
    "\n",
    "model = ViltForQuestionAnswering.from_pretrained(\n",
    "    \"dandelin/vilt-b32-finetuned-vqa\",\n",
    "    num_labels=len(answer_to_id),\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "# Training configuration\n",
    "epochs = 5\n",
    "learning_rate = 5e-5\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_training_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * num_training_steps),\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Epochs: {epochs}\")\n",
    "print(f\"  Learning rate: {learning_rate}\")\n",
    "print(f\"  Training steps: {num_training_steps}\")\n",
    "print(f\"  Warmup steps: {int(0.1 * num_training_steps)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|  | 2/1500 [00:53<11:22:00, 27.32s/it, loss=2098.5903, acc=0.00%]"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    progress_bar = tqdm(loader, desc=\"Training\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # Move to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        # Calculate accuracy (for one-hot labels)\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        targets = batch['labels'].argmax(dim=-1)\n",
    "        correct += (predictions == targets).sum().item()\n",
    "        total += predictions.size(0)\n",
    "        \n",
    "        # Update progress\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100*correct/total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    return total_loss / len(loader), 100 * correct / total\n",
    "\n",
    "\n",
    "def validate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    progress_bar = tqdm(loader, desc=\"Validation\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Calculate accuracy (for one-hot labels)\n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "            targets = batch['labels'].argmax(dim=-1)\n",
    "            correct += (predictions == targets).sum().item()\n",
    "            total += predictions.size(0)\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100*correct/total:.2f}%'\n",
    "            })\n",
    "    \n",
    "    return total_loss / len(loader), 100 * correct / total\n",
    "\n",
    "\n",
    "# Training loop\n",
    "\n",
    "best_val_acc = 0\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate(model, val_loader, device)\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "        }, 'best_vilt_vqa_model.pth')\n",
    "        print(f\"Saved best model (Val Acc: {val_acc:.2f}%)\")\n",
    "\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss\n",
    "ax1.plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "ax1.plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy\n",
    "ax2.plot(history['train_acc'], label='Train Acc', marker='o')\n",
    "ax2.plot(history['val_acc'], label='Val Acc', marker='s')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"  Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"  Final Train Accuracy: {history['train_acc'][-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load('best_vilt_vqa_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(\"Sample Predictions:\\n\")\n",
    "\n",
    "# Test on validation samples\n",
    "for i in range(min(15, len(val_data))):\n",
    "    item = val_data[i]\n",
    "    \n",
    "    try:\n",
    "        image = Image.open(item['image_path']).convert('RGB')\n",
    "        encoding = processor(image, item['question'], return_tensors=\"pt\")\n",
    "        encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoding)\n",
    "            pred_idx = outputs.logits.argmax(dim=-1).item()\n",
    "        \n",
    "        pred_answer = id_to_answer.get(pred_idx, \"unknown\")\n",
    "        is_correct = pred_answer.lower() in [a.lower() for a in item['all_answers']]\n",
    "        \n",
    "        print(f\"{i+1}. Q: {item['question']}\")\n",
    "        print(f\"   Predicted: {pred_answer}\")\n",
    "        print(f\"   Ground Truth: {item['answer']}\")\n",
    "        print(f\"   {'✓ CORRECT' if is_correct else '✗ WRONG'}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"{i+1}. Error: {e}\\n\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nModel saved as: best_vilt_vqa_model.pth\")\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (vqa_tf)",
   "language": "python",
   "name": "vqa_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
