{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Install dependencies (Run first in Colab)\n",
    "# BLIP fast processors need transformers >=4.46\n",
    "!pip install -q \"transformers>=4.46.0\" torch torchvision pillow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Mount Google Drive (Run second in Colab)\n",
    "''''from google.colab import drive\n",
    "drive.mount('/content/drive')''''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Question Answering with ViLT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Transformers imports\n",
    "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Load MS-COCO Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load questions\n",
    "\n",
    "with open('v2_OpenEnded_mscoco_val2014_questions.json', 'r') as f:\n",
    "    questions_data = json.load(f)\n",
    "\n",
    "# Load annotations\n",
    "with open('v2_mscoco_val2014_annotations.json', 'r') as f:\n",
    "    annotations_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(questions_data['questions'])} questions\")\n",
    "print(f\"Loaded {len(annotations_data['annotations'])} annotations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Prepare Dataset for ViLT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare image-question-answer triplets\n",
    "N = 30000  \n",
    "vilt_data = []\n",
    "\n",
    "for i in tqdm(range(min(N, len(questions_data['questions'])))):\n",
    "    question_obj = questions_data['questions'][i]\n",
    "    image_id = question_obj['image_id']\n",
    "    question_id = question_obj['question_id']\n",
    "    img_filename = f\"COCO_val2014_{image_id:012d}.jpg\"\n",
    "    img_path = f\"val2014/{img_filename}\"\n",
    "    \n",
    "    # Check if image exists\n",
    "    if os.path.exists(img_path):\n",
    "        # Get annotation\n",
    "        ann = next((a for a in annotations_data['annotations'] \n",
    "                   if a['question_id'] == question_id), None)\n",
    "        if ann:\n",
    "            # Get most common answer\n",
    "            answers = [ans['answer'] for ans in ann['answers']]\n",
    "            most_common = max(set(answers), key=answers.count)\n",
    "            \n",
    "            vilt_data.append({\n",
    "                'image_path': img_path,\n",
    "                'question': question_obj['question'],\n",
    "                'answer': most_common,\n",
    "                'all_answers': answers\n",
    "            })\n",
    "\n",
    "print(f\"\\n Collected {len(vilt_data)} valid samples\")\n",
    "\n",
    "# Split into train/val (80/20)\n",
    "split_idx = int(0.8 * len(vilt_data))\n",
    "train_data = vilt_data[:split_idx]\n",
    "val_data = vilt_data[split_idx:]\n",
    "\n",
    "print(f\"  Training: {len(train_data)} samples\")\n",
    "print(f\"  Validation: {len(val_data)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Build Answer Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build answer vocabulary from training data\n",
    "train_answers = [d['answer'] for d in train_data]\n",
    "answer_freq = Counter(train_answers)\n",
    "\n",
    "# Use top 3000 answers for better coverage\n",
    "top_k = 3000\n",
    "top_answers = [ans for ans, _ in answer_freq.most_common(top_k)]\n",
    "answer_to_id = {ans: idx for idx, ans in enumerate(top_answers)}\n",
    "id_to_answer = {idx: ans for ans, idx in answer_to_id.items()}\n",
    "\n",
    "print(f\"Answer vocabulary: {len(answer_to_id)} classes\")\n",
    "coverage = sum(1 for d in train_data if d['answer'] in answer_to_id) / len(train_data)\n",
    "print(f\"  Coverage: {coverage*100:.1f}%\")\n",
    "\n",
    "# Show distribution\n",
    "print(f\"\\nTop 10 most common answers:\")\n",
    "for ans, count in answer_freq.most_common(10):\n",
    "    print(f\"  {ans}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: PyTorch Dataset for ViLT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQAViLTDataset(Dataset):\n",
    "    \"\"\"Dataset for ViLT model\"\"\"\n",
    "    def __init__(self, data, processor, answer_to_id):\n",
    "        self.data = data\n",
    "        self.processor = processor\n",
    "        self.answer_to_id = answer_to_id\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        try:\n",
    "            # Load image and RESIZE to fixed size\n",
    "            image = Image.open(item['image_path']).convert('RGB')\n",
    "            image = image.resize((384, 384))  # <-- ADD THIS LINE\n",
    "        except:\n",
    "            # Fallback to blank image\n",
    "            image = Image.new('RGB', (384, 384), color='white')  # <-- Also update this\n",
    "        \n",
    "        question = item['question']\n",
    "        answer = item['answer']\n",
    "        \n",
    "        # Process with ViLT processor\n",
    "        encoding = self.processor(\n",
    "            image, \n",
    "            question,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=40,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Remove batch dimension\n",
    "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "        \n",
    "        # Add label as integer class id (ViLT expects class indices)\n",
    "        label_id = self.answer_to_id.get(answer, -100)\n",
    "        encoding['labels'] = torch.tensor(label_id, dtype=torch.long)\n",
    "        \n",
    "        return encoding\n",
    "        \n",
    "        \n",
    "\n",
    "# Load ViLT processor\n",
    "\n",
    "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = VQAViLTDataset(train_data, processor, answer_to_id)\n",
    "val_dataset = VQAViLTDataset(val_data, processor, answer_to_id)\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"Val dataset: {len(val_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 5.1: BLIP Model Setup\n",
    "\n",
    "# Setup device for BLIP (can use the same as ViLT)\n",
    "device_blip = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "processor_blip = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-capfilt-large\")\n",
    "model_blip = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-vqa-capfilt-large\").to(device_blip)\n",
    "print(\"BLIP model loaded on\", device_blip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function to handle batching\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([item['pixel_values'] for item in batch]),\n",
    "        'input_ids': torch.stack([item['input_ids'] for item in batch]),\n",
    "        'attention_mask': torch.stack([item['attention_mask'] for item in batch]),\n",
    "        'labels': torch.stack([item['labels'] for item in batch])  # [batch]\n",
    "    }\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 16  \n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn \n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn  \n",
    ")\n",
    "\n",
    "print(f\" Train batches: {len(train_loader)}\")\n",
    "print(f\" Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Load ViLT Model and Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained ViLT model\n",
    "\n",
    "model = ViltForQuestionAnswering.from_pretrained(\n",
    "    \"dandelin/vilt-b32-finetuned-vqa\",\n",
    "    num_labels=len(answer_to_id),\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "# Training configuration\n",
    "epochs = 5\n",
    "learning_rate = 5e-5\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_training_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * num_training_steps),\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Epochs: {epochs}\")\n",
    "print(f\"  Learning rate: {learning_rate}\")\n",
    "print(f\"  Training steps: {num_training_steps}\")\n",
    "print(f\"  Warmup steps: {int(0.1 * num_training_steps)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train_epoch(model, loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    progress_bar = tqdm(loader, desc=\"Training\")\n",
    "    \n",
    "    for i, batch in enumerate(progress_bar):\n",
    "        t1 = time.time()\n",
    "        # Move to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        t2 = time.time()\n",
    "        if i < 3:\n",
    "            print(f\"Batch {i} load+to(device): {t2-t1:.2f} seconds\")\n",
    "\n",
    "        # Forward pass\n",
    "        t3 = time.time()\n",
    "        outputs = model(**batch)\n",
    "        t4 = time.time()\n",
    "        if i < 3:\n",
    "            print(f\"Batch {i} forward: {t4-t3:.2f} seconds\")\n",
    "\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        t5 = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        t6 = time.time()\n",
    "        if i < 3:\n",
    "            print(f\"Batch {i} backward: {t6-t5:.2f} seconds\")\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        t7 = time.time()\n",
    "        if i < 3:\n",
    "            print(f\"Batch {i} optimizer+scheduler: {t7-t6:.2f} seconds\")\n",
    "        \n",
    "        # Calculate accuracy (labels are class indices)\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        targets = batch['labels']\n",
    "        correct += (predictions == targets).sum().item()\n",
    "        total += predictions.size(0)\n",
    "        \n",
    "        # Update progress\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100*correct/total:.2f}%'\n",
    "        })\n",
    "        if i == 2:\n",
    "            print(\"Stopping after 3 batches for timing diagnostics.\")\n",
    "            break  # Exit after timing info for 3 batches\n",
    "    \n",
    "    return total_loss / (i+1), 100 * correct / total\n",
    "\n",
    "\n",
    "def validate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    progress_bar = tqdm(loader, desc=\"Validation\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Calculate accuracy (labels are class indices)\n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "            targets = batch['labels']\n",
    "            correct += (predictions == targets).sum().item()\n",
    "            total += predictions.size(0)\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100*correct/total:.2f}%'\n",
    "            })\n",
    "    \n",
    "    return total_loss / len(loader), 100 * correct / total\n",
    "\n",
    "\n",
    "# Training loop\n",
    "\n",
    "best_val_acc = 0\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate(model, val_loader, device)\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "        }, 'best_vilt_vqa_model.pth')\n",
    "        print(f\"Saved best model (Val Acc: {val_acc:.2f}%)\")\n",
    "\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss\n",
    "ax1.plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "ax1.plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy\n",
    "ax2.plot(history['train_acc'], label='Train Acc', marker='o')\n",
    "ax2.plot(history['val_acc'], label='Val Acc', marker='s')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"  Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"  Final Train Accuracy: {history['train_acc'][-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load('best_vilt_vqa_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(\"Sample Predictions:\\n\")\n",
    "\n",
    "# Test on validation samples\n",
    "for i in range(min(15, len(val_data))):\n",
    "    item = val_data[i]\n",
    "    \n",
    "    try:\n",
    "        image = Image.open(item['image_path']).convert('RGB')\n",
    "        encoding = processor(image, item['question'], return_tensors=\"pt\")\n",
    "        encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoding)\n",
    "            pred_idx = outputs.logits.argmax(dim=-1).item()\n",
    "        \n",
    "        pred_answer = id_to_answer.get(pred_idx, \"unknown\")\n",
    "        is_correct = pred_answer.lower() in [a.lower() for a in item['all_answers']]\n",
    "        \n",
    "        print(f\"{i+1}. Q: {item['question']}\")\n",
    "        print(f\"   Predicted: {pred_answer}\")\n",
    "        print(f\"   Ground Truth: {item['answer']}\")\n",
    "        print(f\"   {'✓ CORRECT' if is_correct else '✗ WRONG'}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"{i+1}. Error: {e}\\n\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nModel saved as: best_vilt_vqa_model.pth\")\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 10.1: BLIP Test Predictions on MSCOCO val\n",
    "\n",
    "print(\"BLIP Sample Predictions:\\n\")\n",
    "\n",
    "for i in range(min(15, len(val_data))):\n",
    "    item = val_data[i]\n",
    "    try:\n",
    "        image = Image.open(item['image_path']).convert('RGB')\n",
    "        question = item['question']\n",
    "        \n",
    "        blip_inputs = processor_blip(image, question, return_tensors=\"pt\").to(device_blip)\n",
    "        with torch.no_grad():\n",
    "            output = model_blip.generate(**blip_inputs, max_length=20)\n",
    "        blip_answer = processor_blip.decode(output[0], skip_special_tokens=True)\n",
    "        \n",
    "        ground_truth = item['answer']\n",
    "        is_correct = blip_answer.lower() in [a.lower() for a in item['all_answers']]\n",
    "        \n",
    "        print(f\"{i+1}. Q: {question}\")\n",
    "        print(f\"   BLIP Predicted: {blip_answer}\")\n",
    "        print(f\"   Ground Truth: {ground_truth}\")\n",
    "        print(f\"   {'✓ CORRECT' if is_correct else '✗ WRONG'}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"{i+1}. Error: {e}\\n\")\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
